apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ template "llama-cpp.fullname" . }}
  namespace: {{ template "llama-cpp.namespace" .}}
  labels:
    {{- include "llama-cpp.labels" . | nindent 4 }}
    {{- with .Values.deployment.labels }}
    {{- toYaml . | nindent 4 }}
    {{- end }}
  annotations:
    {{- with .Values.deployment.annotations }}
    {{- toYaml . | nindent 4 }}
    {{- end }}
spec:
  replicas: 1
  selector:
    matchLabels:
      {{- include "llama-cpp.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      labels:
        {{- include "llama-cpp.labels" . | nindent 8 }}
        {{- with .Values.deployment.podLabels }}
        {{- toYaml . | nindent 8 }}
        {{- end }}
      annotations:
        {{- with .Values.deployment.podAnnotations }}
        {{- toYaml . | nindent 8 }}
        {{- end }}
    spec:
      containers:
      - name: serve-model
        image: {{ .Values.deployment.image.repository }}:{{ .Values.deployment.image.tag }}
        imagePullPolicy: {{ .Values.deployment.image.pullPolicy }}
        command:
        - "./llama-server"
        args:
        - "-m"
        - "/data/{{ .Values.model }}/{{ .Values.modelOptions.quantize.convertOutput }}"
        - "--port"
        - "{{ .Values.service.port }}"
        {{- with .Values.modelOptions }}
        {{- if .metricsEnable }}
        - --metrics
        {{- end }}
        {{- end }}
        ports:
        - containerPort: {{ .Values.service.port }}
        volumeMounts:
        - name: shm
          mountPath: /dev/shm
        - name: model-storage
          mountPath: /data
        resources:
          {{- toYaml .Values.deployment.resources | nindent 10 }}
      initContainers:
      - name: download-model
        image: bitnami/git
        imagePullPolicy: {{ .Values.deployment.image.pullPolicy }}
        command: ["bash", "-c", "if ! test -d /data/{{ .Values.model }}; then git lfs install; git clone https://{{ .Values.huggingFaceUser }}:$(HUGGING_FACE_HUB_TOKEN)@huggingface.co/{{ .Values.model }} /data/{{ .Values.model }}; fi"]
        env:
          - name: HUGGING_FACE_HUB_TOKEN
            valueFrom:
              secretKeyRef:
                name: {{ .Values.huggingFaceToken.secretName }}
                key: {{ .Values.huggingFaceToken.secretKey }}
        volumeMounts:
        - name: shm
          mountPath: /dev/shm
        - name: model-storage
          mountPath: /data
      - name: convert-model
        image: {{ .Values.deployment.image.repository }}:{{ .Values.deployment.image.tag }}
        imagePullPolicy: {{ .Values.deployment.image.pullPolicy }}
        command: ["python3", "./convert_hf_to_gguf.py", "/data/{{ .Values.model }}"]
        volumeMounts:
        - name: shm
          mountPath: /dev/shm
        - name: model-storage
          mountPath: /data
      - name: quantize-model
        image: {{ .Values.deployment.image.repository }}:{{ .Values.deployment.image.tag }}
        imagePullPolicy: {{ .Values.deployment.image.pullPolicy }}
        command: ["./llama-quantize", "/data/{{ .Values.model }}/{{ .Values.modelOptions.quantize.convertInput }}", "/data/{{ .Values.model }}/{{ .Values.modelOptions.quantize.convertOutput }}", "{{ .Values.modelOptions.quantize.convertFormat }}"]
        volumeMounts:
        - name: shm
          mountPath: /dev/shm
        - name: model-storage
          mountPath: /data
      volumes:
      - name: shm
        emptyDir:
          medium: Memory
          sizeLimit: 1Gi
      - name: model-storage
        persistentVolumeClaim:
          claimName: {{ template "llama-cpp.fullname" . }}
      {{- with .Values.deployment.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.deployment.tolerations }}
      tolerations:
        {{- toYaml . | nindent 6 }}
      {{- end }}


