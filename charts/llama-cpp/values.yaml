nameOverride: llama-cpp

fullnameOverride: ""

namespaceOverride: ""

instanceLabelOverride: ""

commonLabels: {}

deployment:
  image:
    repository: ghcr.io/ggerganov/llama.cpp
    tag: full-cuda
    pullPolicy: IfNotPresent

  labels: {}

  podLabels: {}

  annotations: {}

  podAnnotations: {}

  resources:
    limits:
      nvidia.com/gpu: 1
    requests:
      cpu: 2
      memory: 5Gi

  tolerations: []

  affinity: {}

service:
  port: 8080

model: meta-llama/Meta-Llama-3-8B-Instruct

modelOptions:
  quantize:
    convertInput: Meta-Llama-3-8B-Instruct-F16.gguf
    convertOutput: ggml-model-q4_0.gguf
    convertFormat: q4_0
  metricsEnable: true
  benchmark:
    promptNumber: 512
    textGenerationNumber: 128
    batchNumber: 2048
    output: json

modelStorage:
  accessModes:
  - ReadWriteOnce
  size: 100Gi

kube-prometheus-stack:
  grafana:
    plugins:
      - grafana-llm-app

huggingFaceToken:
  secretName: hugging-face-token
  secretKey: token
